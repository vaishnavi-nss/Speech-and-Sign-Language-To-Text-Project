<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Job Interview Simulation</title>
  <style>
    body {
      display: flex;
      flex-direction: row;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      margin: 0;
      height: 100vh;
      background: linear-gradient(90deg, #1e1e2f 0%, #2c2c3d 100%);
      color: white;
    }

    .container {
      flex: 1;
      padding: 20px;
      display: flex;
      flex-direction: column;
      gap: 20px;
    }

    h2 {
      color: #66ffe0;
    }

    textarea {
      background: #2a2a3b;
      color: white;
      border: 1px solid #66ffe0;
      padding: 10px;
      font-size: 20px; /* Increased font size for better visibility */
      width: 100%;
      height: 400px; /* Increased height for larger display */
      resize: none;
    }

    button {
      padding: 10px;
      background: #00ffcc;
      color: black;
      font-weight: bold;
      border: none;
      cursor: pointer;
      border-radius: 8px;
      font-size: 16px;
    }

    #mic-icon {
      font-size: 40px;
      color: red;
      transition: 0.3s ease;
    }

    .active {
      text-shadow: 0 0 20px red;
    }

    #sign-video {
      width: 100%;
      height: auto;
      border: 3px solid #00ffff;
      border-radius: 12px;
    }

    .row {
      display: flex;
      gap: 20px;
      flex: 1;
    }
  </style>
</head>
<body>

  <div class="container">
    <h2>üé§ Interviewer (Speech to Text)</h2>
    <div>
      <button onclick="startSpeechToText()">Start Recording</button>
      <button onclick="stopSpeechToText()">Stop Recording</button>
      <span id="mic-icon">üéôÔ∏è</span>
    </div>
    <textarea id="speech-output" placeholder="Speech will be transcribed here..."></textarea>
  </div>

  <div class="container">
    <h2>üßë‚Äçü¶Ø Candidate (Sign to Text)</h2>
    <video id="sign-video" autoplay muted></video>
    <canvas id="hand-canvas"></canvas>
    <textarea id="sign-output" placeholder="Recognized letters will appear here..."></textarea>
    <button onclick="deleteLastWord()">Delete Last Word</button>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script>
    let recognition;
    const micIcon = document.getElementById('mic-icon');

    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      recognition.continuous = true;
      recognition.interimResults = true;

      recognition.onresult = (event) => {
        let transcript = '';
        for (let i = event.resultIndex; i < event.results.length; i++) {
          transcript += event.results[i][0].transcript;
        }
        document.getElementById('speech-output').value = transcript;
      };

      recognition.onstart = () => micIcon.classList.add('active');
      recognition.onend = () => micIcon.classList.remove('active');
    }

    function startSpeechToText() {
      if (recognition) recognition.start();
    }

    function stopSpeechToText() {
      if (recognition) recognition.stop();
    }

    const videoElement = document.getElementById('sign-video');
    const canvasElement = document.getElementById('hand-canvas');
    const ctx = canvasElement.getContext('2d');
    const signOutput = document.getElementById('sign-output');
    let lastDetectionTime = Date.now();
    let currentWord = '';

    function deleteLastWord() {
      let text = signOutput.value.trim();
      let words = text.split(' ');
      words.pop();
      signOutput.value = words.join(' ') + ' ';
    }

    function onResults(results) {
      canvasElement.width = videoElement.videoWidth;
      canvasElement.height = videoElement.videoHeight;

      if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
        const landmarks = results.multiHandLandmarks[0];

        // Draw hand landmarks
        ctx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        ctx.beginPath();
        for (let i = 0; i < landmarks.length; i++) {
          const x = landmarks[i].x * canvasElement.width;
          const y = landmarks[i].y * canvasElement.height;
          ctx.arc(x, y, 5, 0, 2 * Math.PI);
          ctx.fillStyle = "#66ffe0";
          ctx.fill();
        }

        // Dummy prediction (use real model here)
        const dummyLetter = ['A', 'B', 'C'][Math.floor(Math.random() * 3)];
        if (Date.now() - lastDetectionTime > 1000) {
          currentWord += dummyLetter;
          signOutput.value = currentWord;
          lastDetectionTime = Date.now();
        }
      } else {
        if (currentWord.length > 0) {
          signOutput.value += ' ';
          currentWord = '';
        }
      }
    }

    const hands = new Hands({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}` });
    hands.setOptions({
      maxNumHands: 1,
      modelComplexity: 1,
      minDetectionConfidence: 0.7,
      minTrackingConfidence: 0.7
    });
    hands.onResults(onResults);

    const camera = new Camera(videoElement, {
      onFrame: async () => {
        await hands.send({ image: videoElement });
      },
      width: 640,
      height: 480
    });
    camera.start();
  </script>
</body>
</html>
